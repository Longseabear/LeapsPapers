
# Improving Pairwise Ranking for Multi-label Image Classification

## Index

1.	[Abstract](#Abstract)
2. Introduce(#Introduce)
3.	Related Work(#Related Work)
4.	Approach
5.	Experiments
6.	[비고]

---
## Abstract
---------

* pairwise ranking loss가 multi-label image classification에서 좋은 결과를 보이고 있음
* 그러나 2가지 문제를 가짐
  * Hinge loss의 기반으로 deep learning architecture 학습이 힘듬 
  * 기존의 top-k (최상위 k개 선택) 과 threshold는 휴리스틱 성향이 강함
* 그래서 다음과 두가지 새롭게 제안하것다
  * Ranking loss인데 smooth하게하자
  * label decision에서 휴리스틱끼를 빼자 (학습해서 골라보자)
    * top-k를 러닝해서 혼내주자
    * threshold도 러닝해서 고르자 (adaptive하게)

[Index바로가기](#index)

---
## Introduce
---------

최근 pairwise ranking loss를 image classification에 적용하자는 연구가 진행됨

만약 개를 추론에 실패했다고 생각해보자(0.2로 추론)

여기서 pairwise ranking loss의 메인아이디어는 0.2로 추론했어도, 다른 label보다 ranking이 높기만 하면 되는거 아니냐는 것

즉, 내가 원하는 labeling의 확률 값이 다른 값 보다 크기만 하면 된다가 pairwise ranking loss의 핵심

그런데, 이전의 여러 연구는 image에 대하여 적용하기 힘들어

우선, threshold가 ranking loss에 어울리지 않아. Hinge 기반은 optimizing이 힘들어.

또한, 기존의 라벨 결정 방법도 잘못되었어.

예를들어, 이미지 내부의 분류 클래스들은 가변적이야(이미지 컨텐츠에 의존해서 수가 바뀜)

그래서 k-top 방식으로 뽑는 것은 자연스럽지 않아.

또한, thresholding 방식도  모든 이미지 픽셀에 대하여 같은 threshold를 적용하는 것은 자연스럽지 않아.


따라서 다음을 주장합니다

*  Treshold smooth하게 하자.
*  러닝해서 고르자

---
## Related Work
---------

* WARP loss

---
## Approach
---------
* 기존 pairwise loss의 물리적의미
  Rank에 위배되는 것을 Linear하게 처리.
  ![수식](http://latex.codecogs.com/gif.latex?Concentration%3D%5Cfrac%7BTotalTemplate%7D%7BTotalVolume%7D)

  
  
#### Label Prediction
---------




