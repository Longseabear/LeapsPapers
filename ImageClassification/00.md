
# Improving Pairwise Ranking for Multi-label Image Classification

## Index

1.	[Abstract](#Abstract)
2. Introduce(#Introduce)
3.	Related Work(#Related Work)
4.	Approach
5.	Experiments
6.	[비고]

---
## Abstract
---------

* pairwise ranking loss가 multi-label image classification에서 좋은 결과를 보이고 있음
* 그러나 2가지 문제를 가짐
  * Hinge loss의 기반으로 deep learning architecture 학습이 힘듬 
  * 기존의 top-k (최상위 k개 선택) 과 threshold는 휴리스틱 성향이 강함
* 그래서 다음과 두가지 새롭게 제안하것다
  * Ranking loss인데 smooth하게하자
  * label decision에서 휴리스틱끼를 빼자 (학습해서 골라보자)
    * top-k를 러닝해서 혼내주자
    * threshold도 러닝해서 고르자 (adaptive하게)

[Index바로가기](#index)

---
## Introduce
---------

최근 pairwise ranking loss를 image classification에 적용하자는 연구가 진행됨

만약 개를 추론에 실패했다고 생각해보자(0.2로 추론)

여기서 pairwise ranking loss의 메인아이디어는 0.2로 추론했어도, 다른 label보다 ranking이 높기만 하면 되는거 아니냐는 것

즉, 내가 원하는 labeling의 확률 값이 다른 값 보다 크기만 하면 된다가 pairwise ranking loss의 핵심

그런데, 이전의 여러 연구는 image에 대하여 적용하기 힘들어

우선, threshold가 ranking loss에 어울리지 않아. Hinge 기반은 optimizing이 힘들어.

또한, 기존의 라벨 결정 방법도 잘못되었어.

예를들어, 이미지 내부의 분류 클래스들은 가변적이야(이미지 컨텐츠에 의존해서 수가 바뀜)

그래서 k-top 방식으로 뽑는 것은 자연스럽지 않아.

또한, thresholding 방식도  모든 이미지 픽셀에 대하여 같은 threshold를 적용하는 것은 자연스럽지 않아.


따라서 다음을 주장합니다

*  Treshold smooth하게 하자.
*  러닝해서 고르자

---
## Related Work
---------

* WARP loss

---
## Approach
---------
* 기존 pairwise loss의 물리적의미

  Rank에 위배되는 것을 Linear하게 처리.

  ![수식](https://latex.codecogs.com/gif.latex?l_%7Brank%7D%20%3D%20%5Csum_%7Bfalse%7D%5Csum_%7Btrue%7D%7Bmax%280%2Ca&plus;f_%7Bfalse%7D-f_%7Btrue%7D%29%7D)

  alpha는 주로 1.0, false는 트레이닝 정답 라벨에 존재하지 않는 class, true는 존재하는 클래스.
 
  두개의 차이가 1.0을 넘는 순간 loss를 부여하겠다. (그 외에는 상관없다!)
  
  alpha를 주는 이유는, 0이면 거의 비슷해도 차이가 없을 것. 즉, false가 0.99999고 true가 1이면 loss가 0일 것이다.(트레이닝이 안됨)
  

* 제안하는 로스의 물리적의미

  ![수식](https://latex.codecogs.com/gif.latex?l_%7Blsep%7D%20%3D%20log%5C%281&plus;%7B%5Csum_%7Bfalse%7D%5Csum_%7Btrue%7D%7B%5Cexp%28f_%7Bfalse%7D-f_%7Btrue%7D%29%7D%20%7D%29)
  
 log-sum-exponential하게 loss를 주자. 물리적 의미는 기존의 alpha를 제거하고 exponential한 방식으로 smooth하게 로스
 
 즉, loss 입장에서는 ranking이 적합하더라도 트레이닝 로스를 준다(물론 작게준다)
 
 차이가 크면 클 수록 더 loss를 준다. (exponential)
 
 납득
 
 하지만 O(N^2)의 우람한 성능으로 word2vec에서 활용한 negative sampling을 진행
 
 물리적 의미는 걍 트레이닝 하기 적합한걸로만 트레이닝하자. ( 너무느리니까 )
 
 **Q**
* 기존의 WARP loss는 랜덤 샘플링 방식으로 Weight를 줬는데? -> 해보니까 안좋던데 우리꺼엔
* BP-MLL하고 뭐가달라? -> 야씨, 그거 대수론적으로 안정성이 부족해

라벨 Decision 문제는 현재 진행중인 연구와 접합점이 없으므로 생략.

-------

 
 
 
 
#### Label Prediction
---------




